{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60105ea7-9a6a-43e5-ad24-04b3dff17e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, Activation, LayerNormalization\n",
    "from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Reshape, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "import time\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92925555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring TensorFlow...\n",
      "Found 1 GPUs\n",
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_12656\\3520861521.py:21: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "GPU memory limited to 80%\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuring TensorFlow...\")\n",
    "# Memory growth needs to be set before GPUs are initialized\n",
    "try:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        print(f\"Found {len(physical_devices)} GPUs\")\n",
    "        for device in physical_devices:\n",
    "            # Don't pre-allocate memory; allocate as-needed\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(f\"Memory growth enabled for {device}\")\n",
    "        \n",
    "        # Only use first GPU to avoid issues\n",
    "        if len(physical_devices) > 1:\n",
    "            tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "            print(f\"Using only GPU: {physical_devices[0]}\")\n",
    "        \n",
    "        # Limit memory usage to a percentage of GPU memory (adjust as needed)\n",
    "        # This prevents TensorFlow from using all GPU memory\n",
    "        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "        print(\"GPU memory limited to 80%\")\n",
    "    else:\n",
    "        print(\"No GPUs found, using CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring GPU: {e}\")\n",
    "    print(\"Falling back to CPU\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU if GPU config fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e227ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9615fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_dir': 'NPPAD',                 # Directory containing processed CSV files\n",
    "    'sequence_length': 18,               # 3 minutes of history (10sec intervals = 18 points)\n",
    "    'prediction_horizon': 1,             # Binary prediction (will accident happen in next 180s)\n",
    "    'k_folds': 3,                        # Number of folds for cross-validation\n",
    "    'batch_size': 64,                    # Batch size for training\n",
    "    'epochs': 15,                        # Maximum number of epochs (reduced to 20)\n",
    "    'patience': 5,                       # Early stopping patience \n",
    "    'tcn_filters': [64, 128, 128],       # Filters for TCN layers\n",
    "    'tcn_kernel_size': 3,                # Kernel size for TCN\n",
    "    'tcn_dilations': [1, 2, 4, 8],       # Dilation rates for TCN\n",
    "    'attention_heads': 4,                # Number of attention heads\n",
    "    'dropout_rate': 0.3,                 # Dropout rate\n",
    "    'learning_rate': 0.001,              # Learning rate\n",
    "    'test_size': 0.2,                    # Proportion of data for testing\n",
    "    'val_size': 0.2,                     # Proportion of training data for validation\n",
    "    'model_dir': 'models',               # Directory to save models\n",
    "    'results_dir': 'results',            # Directory to save results\n",
    "    'class_weight': {0: 1, 1: 2},        # Weight for handling class imbalance\n",
    "    'use_gpu': True,                     # Using GPU with memory management\n",
    "    'sample_size': 1200,                  # Process only 200 files for testing\n",
    "    'verbose': 1,                        # Verbosity level (0=silent, 1=progress bar, 2=one line per epoch)\n",
    "    'data_chunk_size': 10000             # Process data in chunks to avoid memory issues\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f658facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"Create necessary directories for saving models and results\"\"\"\n",
    "    os.makedirs(CONFIG['model_dir'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "    os.makedirs(os.path.join(CONFIG['results_dir'], 'figures'), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7472573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess all CSV data from the NPPAD directory\"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Find all CSV files\n",
    "    all_files = []\n",
    "    for root, _, _ in os.walk(CONFIG['data_dir']):\n",
    "        files = glob.glob(os.path.join(root, '*.csv'))\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    print(f\"Found {len(all_files)} CSV files\")\n",
    "    \n",
    "    # Load a small sample to determine feature dimensionality\n",
    "    sample_df = pd.read_csv(all_files[0])\n",
    "    \n",
    "    # Skip non-feature columns\n",
    "    non_feature_cols = ['TIME', 'label', 'accident_timestamp', 'accident_type']\n",
    "    feature_cols = [col for col in sample_df.columns if col not in non_feature_cols]\n",
    "    \n",
    "    print(f\"Found {len(feature_cols)} feature columns\")\n",
    "    \n",
    "    # Load data in chunks to manage memory\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    accident_types = []\n",
    "    \n",
    "    # Limit the number of files for debugging if sample_size is set\n",
    "    if CONFIG['sample_size'] is not None:\n",
    "        print(f\"Using a sample of {CONFIG['sample_size']} files for testing\")\n",
    "        all_files = all_files[:CONFIG['sample_size']]\n",
    "    \n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            # Skip files with too few rows\n",
    "            if len(df) < CONFIG['sequence_length'] + CONFIG['prediction_horizon']:\n",
    "                continue\n",
    "                \n",
    "            # Extract features and labels\n",
    "            features = df[feature_cols].values\n",
    "            times = df['TIME'].values\n",
    "            labels = df['label'].values\n",
    "            \n",
    "            # Record accident types for analysis\n",
    "            if 1 in labels:\n",
    "                accident_type = df['accident_type'].iloc[0]\n",
    "                if isinstance(accident_type, str):\n",
    "                    accident_types.append(accident_type)\n",
    "            \n",
    "            # Create sequences with sliding window\n",
    "            for i in range(len(df) - CONFIG['sequence_length'] - CONFIG['prediction_horizon'] + 1):\n",
    "                # Ensure we're using 10-second intervals (check TIME column)\n",
    "                if i > 0 and abs(times[i] - times[i-1] - 10.0) > 1e-5:\n",
    "                    continue\n",
    "                \n",
    "                seq = features[i:i+CONFIG['sequence_length']]\n",
    "                \n",
    "                # Label is 1 if any point in the prediction horizon has label 1\n",
    "                target_labels = labels[i+CONFIG['sequence_length']:\n",
    "                                       i+CONFIG['sequence_length']+CONFIG['prediction_horizon']]\n",
    "                target = 1 if 1 in target_labels else 0\n",
    "                \n",
    "                all_sequences.append(seq)\n",
    "                all_labels.append(target)\n",
    "                \n",
    "                # Process in chunks to avoid memory issues\n",
    "                if len(all_sequences) >= CONFIG['data_chunk_size']:\n",
    "                    print(f\"Processed {len(all_sequences)} sequences so far...\")\n",
    "                    chunk_X = np.array(all_sequences)\n",
    "                    chunk_y = np.array(all_labels)\n",
    "                    yield chunk_X, chunk_y, feature_cols, False\n",
    "                    all_sequences = []\n",
    "                    all_labels = []\n",
    "                    gc.collect()  # Force garbage collection\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert remaining sequences to numpy arrays\n",
    "    if all_sequences:\n",
    "        X = np.array(all_sequences)\n",
    "        y = np.array(all_labels)\n",
    "        \n",
    "        print(f\"Created {len(X)} sequences in final chunk\")\n",
    "        print(f\"Class distribution: {Counter(y)}\")\n",
    "        \n",
    "        # Print accident type distribution\n",
    "        if accident_types:\n",
    "            print(\"Accident type distribution:\")\n",
    "            for acc_type, count in Counter(accident_types).items():\n",
    "                print(f\"  {acc_type}: {count}\")\n",
    "        \n",
    "        yield X, y, feature_cols, True\n",
    "    else:\n",
    "        print(\"No sequences created in final chunk\")\n",
    "        yield None, None, feature_cols, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67eae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, dilation_rate, nb_filters, kernel_size, dropout_rate):\n",
    "    \"\"\"TCN residual block with dilated causal convolutions\"\"\"\n",
    "    prev_x = x\n",
    "    \n",
    "    # Layer normalization\n",
    "    x = LayerNormalization()(x)\n",
    "    \n",
    "    # Dilated causal convolution\n",
    "    x = Conv1D(filters=nb_filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate,\n",
    "               activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Second dilated causal convolution\n",
    "    x = Conv1D(filters=nb_filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate,\n",
    "               activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # If dimensions don't match, transform the input\n",
    "    if prev_x.shape[-1] != nb_filters:\n",
    "        prev_x = Conv1D(nb_filters, 1, padding='same')(prev_x)\n",
    "    \n",
    "    # Residual connection\n",
    "    res = prev_x + x\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2832e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(x, num_heads, key_dim):\n",
    "    \"\"\"Multi-head self-attention block\"\"\"\n",
    "    # Self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=key_dim\n",
    "    )(x, x)\n",
    "    \n",
    "    # Skip connection\n",
    "    return x + attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341d9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_tcn_attention_model(input_shape):\n",
    "    \"\"\"Build TCN model with attention mechanism\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # TCN blocks with increasing dilation rates\n",
    "    for i, (nb_filters, dilation_rate) in enumerate(\n",
    "            zip(CONFIG['tcn_filters'], CONFIG['tcn_dilations'])):\n",
    "        x = residual_block(\n",
    "            x, \n",
    "            dilation_rate=dilation_rate,\n",
    "            nb_filters=nb_filters,\n",
    "            kernel_size=CONFIG['tcn_kernel_size'],\n",
    "            dropout_rate=CONFIG['dropout_rate']\n",
    "        )\n",
    "    \n",
    "    # Attention mechanism\n",
    "    x = attention_block(x, CONFIG['attention_heads'], key_dim=CONFIG['tcn_filters'][-1]//CONFIG['attention_heads'])\n",
    "    \n",
    "    # Global pooling to reduce sequence dimension\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model with mixed precision for faster GPU training\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n",
    "    \n",
    "    # Use mixed precision if on GPU\n",
    "    if CONFIG['use_gpu'] and tf.config.list_physical_devices('GPU'):\n",
    "        # Mixed precision uses float16 for most ops but keeps float32 for critical ops\n",
    "        print(\"Using mixed precision for faster GPU training\")\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.AUC()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b36696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, fold=None):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    # Plot precision\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['precision'])\n",
    "    plt.plot(history.history['val_precision'])\n",
    "    plt.title('Model Precision')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot recall\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['recall'])\n",
    "    plt.plot(history.history['val_recall'])\n",
    "    plt.title('Model Recall')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt_path = os.path.join(CONFIG['results_dir'], 'figures', \n",
    "                           f'training_history{\"_fold\"+str(fold) if fold is not None else \"\"}.png')\n",
    "    plt.savefig(plt_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6939c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, fold=None):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    # Convert lists to numpy arrays if they aren't already\n",
    "    y_true_arr = np.array(y_true)\n",
    "    y_pred_arr = np.array(y_pred)\n",
    "    \n",
    "    # Apply threshold to predictions\n",
    "    y_pred_binary = (y_pred_arr > 0.5).astype(int)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true_arr, y_pred_binary)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Accident'],\n",
    "                yticklabels=['Normal', 'Accident'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Save figure\n",
    "    plt_path = os.path.join(CONFIG['results_dir'], 'figures', \n",
    "                           f'confusion_matrix{\"_fold\"+str(fold) if fold is not None else \"\"}.png')\n",
    "    plt.savefig(plt_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3c2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data_chunks(data_chunks):\n",
    "    \"\"\"Combine data chunks into a single numpy array\"\"\"\n",
    "    if not data_chunks:\n",
    "        return None, None\n",
    "        \n",
    "    X_chunks = [chunk[0] for chunk in data_chunks if chunk[0] is not None]\n",
    "    y_chunks = [chunk[1] for chunk in data_chunks if chunk[1] is not None]\n",
    "    \n",
    "    if not X_chunks or not y_chunks:\n",
    "        return None, None\n",
    "        \n",
    "    X = np.concatenate(X_chunks, axis=0)\n",
    "    y = np.concatenate(y_chunks, axis=0)\n",
    "    \n",
    "    print(f\"Combined data shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "    print(f\"Final class distribution: {Counter(y)}\")\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992a0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_fold(fold, X, y, train_idx, val_idx, all_val_predictions, all_val_true):\n",
    "    \"\"\"Helper function to train a single fold\"\"\"\n",
    "    # Split data\n",
    "    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Scale features using Min-Max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_fold_reshaped = X_train_fold.reshape(-1, X_train_fold.shape[-1])\n",
    "    X_val_fold_reshaped = X_val_fold.reshape(-1, X_val_fold.shape[-1])\n",
    "    \n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold_reshaped)\n",
    "    X_val_fold_scaled = scaler.transform(X_val_fold_reshaped)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_train_fold = X_train_fold_scaled.reshape(X_train_fold.shape)\n",
    "    X_val_fold = X_val_fold_scaled.reshape(X_val_fold.shape)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_tcn_attention_model((X_train_fold.shape[1], X_train_fold.shape[2]))\n",
    "    \n",
    "    if fold == 0:\n",
    "        # Print model summary for the first fold\n",
    "        model.summary()\n",
    "        try:\n",
    "            plot_model(model, to_file=os.path.join(CONFIG['results_dir'], 'model_architecture.png'), \n",
    "                       show_shapes=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate model plot: {e}\")\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=CONFIG['patience'],\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(CONFIG['model_dir'], f'model_fold_{fold+1}.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=CONFIG['class_weight'],\n",
    "        verbose=CONFIG['verbose']\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, fold+1)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc, val_precision, val_recall, val_auc = model.evaluate(\n",
    "        X_val_fold, y_val_fold, verbose=0)\n",
    "    \n",
    "    # Get predictions\n",
    "    val_pred = model.predict(X_val_fold, batch_size=CONFIG['batch_size'], verbose=0)\n",
    "    \n",
    "    # Flatten prediction array before extending the list\n",
    "    val_pred_flat = val_pred.flatten()\n",
    "    all_val_predictions.extend(val_pred_flat.tolist())\n",
    "    all_val_true.extend(y_val_fold.tolist())\n",
    "    \n",
    "    # Plot confusion matrix for this fold\n",
    "    try:\n",
    "        plot_confusion_matrix(y_val_fold, val_pred, fold+1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting confusion matrix for fold {fold+1}: {e}\")\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_result = {\n",
    "        'fold': fold + 1,\n",
    "        'val_loss': float(val_loss),\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'val_precision': float(val_precision),\n",
    "        'val_recall': float(val_recall),\n",
    "        'val_auc': float(val_auc),\n",
    "        'training_time': train_time,\n",
    "        'best_epoch': len(history.history['loss']) - CONFIG['patience']\n",
    "    }\n",
    "    \n",
    "    return fold_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a6c21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_kfold(all_data):\n",
    "    \"\"\"Train the model with k-fold cross-validation\"\"\"\n",
    "    print(f\"Starting {CONFIG['k_folds']}-fold cross-validation...\")\n",
    "    \n",
    "    # Combine all data chunks into X and y\n",
    "    X, y = combine_data_chunks(all_data)\n",
    "    \n",
    "    if X is None or len(X) == 0:\n",
    "        print(\"No data to train on!\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize k-fold\n",
    "    kfold = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    fold_results = []\n",
    "    all_val_predictions = []\n",
    "    all_val_true = []\n",
    "    \n",
    "    # Train and evaluate for each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{CONFIG['k_folds']}\")\n",
    "        \n",
    "        tf.keras.backend.clear_session()  # Clear memory between folds\n",
    "        \n",
    "        # Train fold with error handling\n",
    "        try:\n",
    "            fold_result = _train_fold(fold, X, y, train_idx, val_idx, all_val_predictions, all_val_true)\n",
    "            fold_results.append(fold_result)\n",
    "            print(f\"Fold {fold+1} results: {fold_result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold+1}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    if all_val_true and all_val_predictions:\n",
    "        # Convert lists to numpy arrays\n",
    "        all_val_true_arr = np.array(all_val_true)\n",
    "        all_val_predictions_arr = np.array(all_val_predictions)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        overall_auc = roc_auc_score(all_val_true_arr, all_val_predictions_arr)\n",
    "        \n",
    "        # Calculate binary predictions\n",
    "        binary_predictions = (all_val_predictions_arr > 0.5).astype(int)\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(all_val_true_arr, binary_predictions, output_dict=True)\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'config': {k: str(v) if isinstance(v, (dict, list)) else v for k, v in CONFIG.items()},\n",
    "            'fold_results': fold_results,\n",
    "            'overall_auc': float(overall_auc),\n",
    "            'classification_report': report\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(CONFIG['results_dir'], 'kfold_results.json'), 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Plot overall confusion matrix\n",
    "        plot_confusion_matrix(all_val_true, all_val_predictions)\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"No validation results collected, cannot compute overall metrics\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5464a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bcd7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_final_model_internal(X, y):\n",
    "    \"\"\"Internal function for training the final model (to be used with device context)\"\"\"\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG['test_size'], random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split train into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=CONFIG['val_size'], random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "    X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "    X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "    X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_train = X_train_scaled.reshape(X_train.shape)\n",
    "    X_val = X_val_scaled.reshape(X_val.shape)\n",
    "    X_test = X_test_scaled.reshape(X_test.shape)\n",
    "    \n",
    "    # Save scaler for future use\n",
    "    import joblib\n",
    "    joblib.dump(scaler, os.path.join(CONFIG['model_dir'], 'scaler.pkl'))\n",
    "    \n",
    "    # Build model\n",
    "    model = build_tcn_attention_model((X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=CONFIG['patience'],\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(CONFIG['model_dir'], 'final_model.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=CONFIG['class_weight'],\n",
    "        verbose=CONFIG['verbose']\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_precision, test_recall, test_auc = model.evaluate(\n",
    "        X_test, y_test, batch_size=CONFIG['batch_size'], verbose=0\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    test_pred = model.predict(X_test, batch_size=CONFIG['batch_size'], verbose=0)\n",
    "    \n",
    "    # Plot confusion matrix - ensure arrays are properly formatted\n",
    "    try:\n",
    "        plot_confusion_matrix(y_test, test_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting confusion matrix for final model: {e}\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    test_pred_np = np.array(test_pred)\n",
    "    binary_predictions = (test_pred_np > 0.5).astype(int)\n",
    "    \n",
    "    report = classification_report(y_test, binary_predictions, output_dict=True)\n",
    "    \n",
    "    # Save test results\n",
    "    test_results = {\n",
    "        'test_loss': float(test_loss),\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_auc': float(test_auc),\n",
    "        'classification_report': report\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(CONFIG['results_dir'], 'test_results.json'), 'w') as f:\n",
    "        json.dump(test_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Test results: {test_results}\")\n",
    "    \n",
    "    # Save model in SavedModel format for deployment\n",
    "    model.save(os.path.join(CONFIG['model_dir'], 'final_model_saved'))\n",
    "    \n",
    "    print(\"Final model training complete!\")\n",
    "    \n",
    "    return model, test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c09d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(all_data):\n",
    "    \"\"\"Train final model on all data with a proper test split\"\"\"\n",
    "    print(\"\\nTraining final model...\")\n",
    "    \n",
    "    # Combine all data chunks into X and y\n",
    "    X, y = combine_data_chunks(all_data)\n",
    "    \n",
    "    if X is None or len(X) == 0:\n",
    "        print(\"No data to train final model on!\")\n",
    "        return None, None\n",
    "        \n",
    "    tf.keras.backend.clear_session()  # Clear memory before final model\n",
    "    \n",
    "    try:\n",
    "        return _train_final_model_internal(X, y)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in final model training: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb8b26-90d9-4e6a-88d0-2d61d6d1da28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d1009d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to execute the training pipeline\"\"\"\n",
    "    # Create directories\n",
    "    create_directories()\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess data in chunks\n",
    "        all_data = []\n",
    "        feature_cols = None\n",
    "        \n",
    "        # Process data in chunks to avoid memory issues\n",
    "        for X_chunk, y_chunk, cols, is_last_chunk in load_and_preprocess_data():\n",
    "            if X_chunk is not None and y_chunk is not None:\n",
    "                all_data.append((X_chunk, y_chunk))\n",
    "                \n",
    "                if feature_cols is None:\n",
    "                    feature_cols = cols\n",
    "            \n",
    "            # Break if we're just testing with a small sample\n",
    "            if is_last_chunk:\n",
    "                break\n",
    "                \n",
    "        if not all_data:\n",
    "            print(\"No data was loaded!\")\n",
    "            return\n",
    "            \n",
    "        # Save feature columns for future reference\n",
    "        if feature_cols:\n",
    "            with open(os.path.join(CONFIG['model_dir'], 'feature_columns.json'), 'w') as f:\n",
    "                json.dump(feature_cols, f)\n",
    "        \n",
    "        # Train with k-fold cross-validation\n",
    "        try:\n",
    "            print(\"Starting cross-validation...\")\n",
    "            kfold_results = train_with_kfold(all_data)\n",
    "            print(\"Cross-validation completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cross-validation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            kfold_results = None\n",
    "        \n",
    "        # Train final model\n",
    "        try:\n",
    "            print(\"Starting final model training...\")\n",
    "            final_model, test_results = train_final_model(all_data)\n",
    "            print(\"Final model training completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during final model training: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            final_model, test_results = None, None\n",
    "        \n",
    "        print(\"\\nTraining pipeline complete!\")\n",
    "        if kfold_results:\n",
    "            print(f\"Overall AUC across folds: {kfold_results.get('overall_auc', 'N/A')}\")\n",
    "        \n",
    "        if test_results:\n",
    "            print(f\"Final model test accuracy: {test_results.get('test_accuracy', 'N/A')}\")\n",
    "            print(f\"Final model test AUC: {test_results.get('test_auc', 'N/A')}\")\n",
    "        \n",
    "        return final_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09498fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Found 1211 CSV files\n",
      "Found 96 feature columns\n",
      "Using a sample of 1200 files for testing\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Processed 10000 sequences so far...\n",
      "Created 3151 sequences in final chunk\n",
      "Class distribution: Counter({0: 3151})\n",
      "Accident type distribution:\n",
      "  Reactor Scram: 660\n",
      "Starting cross-validation...\n",
      "Starting 3-fold cross-validation...\n",
      "Combined data shape: (463151, 18, 96), Labels shape: (463151,)\n",
      "Final class distribution: Counter({0: 281372, 1: 181779})\n",
      "\n",
      "Training fold 1/3\n",
      "Using mixed precision for faster GPU training\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 18, 96)]     0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 18, 96)      192         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 18, 64)       18496       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 18, 64)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 18, 64)       12352       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 18, 64)       6208        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 18, 64)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 18, 64)      0           ['conv1d_2[0][0]',               \n",
      " da)                                                              'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 18, 64)      128         ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 18, 128)      24704       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 18, 128)      0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 18, 128)      49280       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 18, 128)      8320        ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 18, 128)      0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 18, 128)     0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 18, 128)     256         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 18, 128)      49280       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 18, 128)      0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 18, 128)      49280       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 18, 128)      0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 18, 128)     0           ['tf.__operators__.add_1[0][0]', \n",
      " mbda)                                                            'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 18, 128)     66048       ['tf.__operators__.add_2[0][0]', \n",
      " dAttention)                                                      'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 18, 128)     0           ['tf.__operators__.add_2[0][0]', \n",
      " mbda)                                                            'multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['tf.__operators__.add_3[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            129         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 284,673\n",
      "Trainable params: 284,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/15\n",
      "4825/4825 [==============================] - 165s 32ms/step - loss: 0.0341 - accuracy: 0.9919 - precision: 0.9902 - recall: 0.9893 - auc: 0.9994 - val_loss: 0.0207 - val_accuracy: 0.9923 - val_precision: 0.9920 - val_recall: 0.9882 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "4825/4825 [==============================] - 147s 30ms/step - loss: 0.0205 - accuracy: 0.9943 - precision: 0.9915 - recall: 0.9940 - auc: 0.9998 - val_loss: 0.0094 - val_accuracy: 0.9958 - val_precision: 0.9988 - val_recall: 0.9906 - val_auc: 1.0000 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "4825/4825 [==============================] - 147s 30ms/step - loss: 0.0183 - accuracy: 0.9950 - precision: 0.9923 - recall: 0.9949 - auc: 0.9998 - val_loss: 0.0200 - val_accuracy: 0.9937 - val_precision: 0.9992 - val_recall: 0.9845 - val_auc: 0.9990 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "4825/4825 [==============================] - 146s 30ms/step - loss: 0.0200 - accuracy: 0.9950 - precision: 0.9924 - recall: 0.9948 - auc: 0.9997 - val_loss: 0.0095 - val_accuracy: 0.9957 - val_precision: 0.9998 - val_recall: 0.9891 - val_auc: 1.0000 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "4825/4825 [==============================] - 147s 30ms/step - loss: 0.0178 - accuracy: 0.9949 - precision: 0.9925 - recall: 0.9945 - auc: 0.9998 - val_loss: 0.0079 - val_accuracy: 0.9967 - val_precision: 0.9941 - val_recall: 0.9976 - val_auc: 1.0000 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "4825/4825 [==============================] - 147s 30ms/step - loss: 0.0136 - accuracy: 0.9962 - precision: 0.9939 - recall: 0.9965 - auc: 0.9998 - val_loss: 0.0216 - val_accuracy: 0.9931 - val_precision: 0.9998 - val_recall: 0.9825 - val_auc: 0.9998 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "4825/4825 [==============================] - 147s 30ms/step - loss: 0.0147 - accuracy: 0.9960 - precision: 0.9938 - recall: 0.9960 - auc: 0.9998 - val_loss: 0.0064 - val_accuracy: 0.9975 - val_precision: 0.9960 - val_recall: 0.9977 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "4825/4825 [==============================] - 147s 31ms/step - loss: 0.0178 - accuracy: 0.9949 - precision: 0.9921 - recall: 0.9948 - auc: 0.9998 - val_loss: 0.0157 - val_accuracy: 0.9943 - val_precision: 0.9876 - val_recall: 0.9978 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "4825/4825 [==============================] - 147s 30ms/step - loss: 0.0163 - accuracy: 0.9953 - precision: 0.9926 - recall: 0.9955 - auc: 0.9998 - val_loss: 0.0092 - val_accuracy: 0.9961 - val_precision: 0.9978 - val_recall: 0.9922 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "4825/4825 [==============================] - 148s 31ms/step - loss: 0.0197 - accuracy: 0.9951 - precision: 0.9921 - recall: 0.9954 - auc: 0.9997 - val_loss: 0.0064 - val_accuracy: 0.9975 - val_precision: 0.9959 - val_recall: 0.9976 - val_auc: 1.0000 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "4825/4825 [==============================] - 147s 31ms/step - loss: 0.0085 - accuracy: 0.9974 - precision: 0.9955 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0049 - val_accuracy: 0.9978 - val_precision: 0.9982 - val_recall: 0.9962 - val_auc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 12/15\n",
      "4825/4825 [==============================] - 148s 31ms/step - loss: 0.0098 - accuracy: 0.9971 - precision: 0.9953 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0081 - val_accuracy: 0.9966 - val_precision: 0.9990 - val_recall: 0.9923 - val_auc: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 13/15\n",
      "4825/4825 [==============================] - 147s 31ms/step - loss: 0.0086 - accuracy: 0.9974 - precision: 0.9957 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0090 - val_accuracy: 0.9958 - val_precision: 0.9924 - val_recall: 0.9969 - val_auc: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 14/15\n",
      "4825/4825 [==============================] - 147s 31ms/step - loss: 0.0120 - accuracy: 0.9967 - precision: 0.9946 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0059 - val_accuracy: 0.9977 - val_precision: 0.9996 - val_recall: 0.9946 - val_auc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 15/15\n",
      "4825/4825 [==============================] - 148s 31ms/step - loss: 0.0070 - accuracy: 0.9979 - precision: 0.9965 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9984 - val_precision: 0.9993 - val_recall: 0.9966 - val_auc: 1.0000 - lr: 2.5000e-04\n",
      "Fold 1 results: {'fold': 1, 'val_loss': 0.0038953458424657583, 'val_accuracy': 0.9984130263328552, 'val_precision': 0.9993031620979309, 'val_recall': 0.9966408014297485, 'val_auc': 0.999983012676239, 'training_time': 2230.9124813079834, 'best_epoch': 10}\n",
      "\n",
      "Training fold 2/3\n",
      "Using mixed precision for faster GPU training\n",
      "Epoch 1/15\n",
      "4825/4825 [==============================] - 159s 32ms/step - loss: 0.0351 - accuracy: 0.9922 - precision: 0.9914 - recall: 0.9889 - auc: 0.9992 - val_loss: 0.0114 - val_accuracy: 0.9957 - val_precision: 0.9945 - val_recall: 0.9946 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "4825/4825 [==============================] - 152s 31ms/step - loss: 0.0219 - accuracy: 0.9943 - precision: 0.9920 - recall: 0.9935 - auc: 0.9996 - val_loss: 0.0124 - val_accuracy: 0.9955 - val_precision: 0.9914 - val_recall: 0.9970 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "4825/4825 [==============================] - 152s 32ms/step - loss: 0.0181 - accuracy: 0.9952 - precision: 0.9930 - recall: 0.9947 - auc: 0.9998 - val_loss: 0.0106 - val_accuracy: 0.9955 - val_precision: 0.9965 - val_recall: 0.9921 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0162 - accuracy: 0.9954 - precision: 0.9928 - recall: 0.9955 - auc: 0.9998 - val_loss: 0.0070 - val_accuracy: 0.9974 - val_precision: 0.9970 - val_recall: 0.9963 - val_auc: 1.0000 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0148 - accuracy: 0.9959 - precision: 0.9937 - recall: 0.9960 - auc: 0.9998 - val_loss: 0.0114 - val_accuracy: 0.9953 - val_precision: 0.9997 - val_recall: 0.9882 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0173 - accuracy: 0.9953 - precision: 0.9932 - recall: 0.9949 - auc: 0.9998 - val_loss: 0.0158 - val_accuracy: 0.9943 - val_precision: 0.9871 - val_recall: 0.9985 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "4825/4825 [==============================] - 152s 32ms/step - loss: 0.0177 - accuracy: 0.9949 - precision: 0.9925 - recall: 0.9946 - auc: 0.9998 - val_loss: 0.0110 - val_accuracy: 0.9960 - val_precision: 0.9953 - val_recall: 0.9945 - val_auc: 0.9998 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "4825/4825 [==============================] - 158s 33ms/step - loss: 0.0119 - accuracy: 0.9966 - precision: 0.9944 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0049 - val_accuracy: 0.9980 - val_precision: 0.9993 - val_recall: 0.9957 - val_auc: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 9/15\n",
      "4825/4825 [==============================] - 152s 32ms/step - loss: 0.0083 - accuracy: 0.9974 - precision: 0.9956 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9977 - val_precision: 0.9959 - val_recall: 0.9984 - val_auc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 10/15\n",
      "4825/4825 [==============================] - 152s 32ms/step - loss: 0.0101 - accuracy: 0.9972 - precision: 0.9955 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0073 - val_accuracy: 0.9969 - val_precision: 0.9980 - val_recall: 0.9941 - val_auc: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 11/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0094 - accuracy: 0.9973 - precision: 0.9957 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0075 - val_accuracy: 0.9971 - val_precision: 0.9990 - val_recall: 0.9936 - val_auc: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 12/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0067 - accuracy: 0.9979 - precision: 0.9965 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0044 - val_accuracy: 0.9982 - val_precision: 0.9996 - val_recall: 0.9958 - val_auc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 13/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0066 - accuracy: 0.9979 - precision: 0.9964 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9986 - val_precision: 0.9991 - val_recall: 0.9975 - val_auc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 14/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0061 - accuracy: 0.9981 - precision: 0.9969 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9972 - val_precision: 0.9998 - val_recall: 0.9931 - val_auc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 15/15\n",
      "4825/4825 [==============================] - 153s 32ms/step - loss: 0.0058 - accuracy: 0.9983 - precision: 0.9970 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9987 - val_precision: 0.9988 - val_recall: 0.9979 - val_auc: 1.0000 - lr: 2.5000e-04\n",
      "Error in fold 2: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n",
      "\n",
      "Training fold 3/3\n",
      "Using mixed precision for faster GPU training\n",
      "Error in fold 3: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n",
      "Cross-validation completed successfully\n",
      "Starting final model training...\n",
      "\n",
      "Training final model...\n",
      "Combined data shape: (463151, 18, 96), Labels shape: (463151,)\n",
      "Final class distribution: Counter({0: 281372, 1: 181779})\n",
      "Train set: (296416, 18, 96), Validation set: (74104, 18, 96), Test set: (92631, 18, 96)\n",
      "Using mixed precision for faster GPU training\n",
      "Error in final model training: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n",
      "Final model training completed successfully\n",
      "\n",
      "Training pipeline complete!\n",
      "Overall AUC across folds: 0.999988111240086\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0209f-c96e-4057-a3c1-5e33e08cede0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71794d9a-e88f-4385-91f0-eccd38fcc08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
