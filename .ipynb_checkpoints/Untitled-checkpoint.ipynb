{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60105ea7-9a6a-43e5-ad24-04b3dff17e0f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, Activation, LayerNormalization\n",
    "from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Reshape, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "import time\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e227ca",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9615fc3f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'data_dir': 'NPPAD',                 # Directory containing processed CSV files\n",
    "    'sequence_length': 18,               # 3 minutes of history (10sec intervals = 18 points)\n",
    "    'prediction_horizon': 1,             # Binary prediction (will accident happen in next 180s)\n",
    "    'k_folds': 5,                        # Number of folds for cross-validation\n",
    "    'batch_size': 64,                    # Batch size for training\n",
    "    'epochs': 100,                       # Maximum number of epochs\n",
    "    'patience': 10,                      # Early stopping patience\n",
    "    'tcn_filters': [64, 128, 128],       # Filters for TCN layers\n",
    "    'tcn_kernel_size': 3,                # Kernel size for TCN\n",
    "    'tcn_dilations': [1, 2, 4, 8],       # Dilation rates for TCN\n",
    "    'attention_heads': 4,                # Number of attention heads\n",
    "    'dropout_rate': 0.3,                 # Dropout rate\n",
    "    'learning_rate': 0.001,              # Learning rate\n",
    "    'test_size': 0.2,                    # Proportion of data for testing\n",
    "    'val_size': 0.2,                     # Proportion of training data for validation\n",
    "    'model_dir': 'models',               # Directory to save models\n",
    "    'results_dir': 'results',            # Directory to save results\n",
    "    'class_weight': {0: 1, 1: 2}         # Weight for handling class imbalance\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f658facc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"Create necessary directories for saving models and results\"\"\"\n",
    "    os.makedirs(CONFIG['model_dir'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "    os.makedirs(os.path.join(CONFIG['results_dir'], 'figures'), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7472573",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess all CSV data from the NPPAD directory\"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Find all CSV files\n",
    "    all_files = []\n",
    "    for root, _, _ in os.walk(CONFIG['data_dir']):\n",
    "        files = glob.glob(os.path.join(root, '*.csv'))\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    print(f\"Found {len(all_files)} CSV files\")\n",
    "    \n",
    "    # Load a small sample to determine feature dimensionality\n",
    "    sample_df = pd.read_csv(all_files[0])\n",
    "    \n",
    "    # Skip non-feature columns\n",
    "    non_feature_cols = ['TIME', 'label', 'accident_timestamp', 'accident_type']\n",
    "    feature_cols = [col for col in sample_df.columns if col not in non_feature_cols]\n",
    "    \n",
    "    print(f\"Found {len(feature_cols)} feature columns\")\n",
    "    \n",
    "    # Load all data\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    accident_types = []\n",
    "    \n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            # Skip files with too few rows\n",
    "            if len(df) < CONFIG['sequence_length'] + CONFIG['prediction_horizon']:\n",
    "                continue\n",
    "                \n",
    "            # Extract features and labels\n",
    "            features = df[feature_cols].values\n",
    "            times = df['TIME'].values\n",
    "            labels = df['label'].values\n",
    "            \n",
    "            # Record accident types for analysis\n",
    "            if 1 in labels:\n",
    "                accident_type = df['accident_type'].iloc[0]\n",
    "                if isinstance(accident_type, str):\n",
    "                    accident_types.append(accident_type)\n",
    "            \n",
    "            # Create sequences with sliding window\n",
    "            for i in range(len(df) - CONFIG['sequence_length'] - CONFIG['prediction_horizon'] + 1):\n",
    "                # Ensure we're using 10-second intervals (check TIME column)\n",
    "                if i > 0 and abs(times[i] - times[i-1] - 10.0) > 1e-5:\n",
    "                    continue\n",
    "                \n",
    "                seq = features[i:i+CONFIG['sequence_length']]\n",
    "                \n",
    "                # Label is 1 if any point in the prediction horizon has label 1\n",
    "                target_labels = labels[i+CONFIG['sequence_length']:\n",
    "                                       i+CONFIG['sequence_length']+CONFIG['prediction_horizon']]\n",
    "                target = 1 if 1 in target_labels else 0\n",
    "                \n",
    "                all_sequences.append(seq)\n",
    "                all_labels.append(target)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(all_sequences)\n",
    "    y = np.array(all_labels)\n",
    "    \n",
    "    print(f\"Created {len(X)} sequences\")\n",
    "    print(f\"Class distribution: {Counter(y)}\")\n",
    "    \n",
    "    # Print accident type distribution\n",
    "    if accident_types:\n",
    "        print(\"Accident type distribution:\")\n",
    "        for acc_type, count in Counter(accident_types).items():\n",
    "            print(f\"  {acc_type}: {count}\")\n",
    "    \n",
    "    return X, y, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67eae1ee",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def residual_block(x, dilation_rate, nb_filters, kernel_size, dropout_rate):\n",
    "    \"\"\"TCN residual block with dilated causal convolutions\"\"\"\n",
    "    prev_x = x\n",
    "    \n",
    "    # Layer normalization\n",
    "    x = LayerNormalization()(x)\n",
    "    \n",
    "    # Dilated causal convolution\n",
    "    x = Conv1D(filters=nb_filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate,\n",
    "               activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Second dilated causal convolution\n",
    "    x = Conv1D(filters=nb_filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate,\n",
    "               activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # If dimensions don't match, transform the input\n",
    "    if prev_x.shape[-1] != nb_filters:\n",
    "        prev_x = Conv1D(nb_filters, 1, padding='same')(prev_x)\n",
    "    \n",
    "    # Residual connection\n",
    "    res = prev_x + x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2832e416",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def attention_block(x, num_heads, key_dim):\n",
    "    \"\"\"Multi-head self-attention block\"\"\"\n",
    "    # Self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=key_dim\n",
    "    )(x, x)\n",
    "    \n",
    "    # Skip connection\n",
    "    return x + attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341d9a49",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def build_tcn_attention_model(input_shape):\n",
    "    \"\"\"Build TCN model with attention mechanism\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # TCN blocks with increasing dilation rates\n",
    "    for i, (nb_filters, dilation_rate) in enumerate(\n",
    "            zip(CONFIG['tcn_filters'], CONFIG['tcn_dilations'])):\n",
    "        x = residual_block(\n",
    "            x, \n",
    "            dilation_rate=dilation_rate,\n",
    "            nb_filters=nb_filters,\n",
    "            kernel_size=CONFIG['tcn_kernel_size'],\n",
    "            dropout_rate=CONFIG['dropout_rate']\n",
    "        )\n",
    "    \n",
    "    # Attention mechanism\n",
    "    x = attention_block(x, CONFIG['attention_heads'], key_dim=CONFIG['tcn_filters'][-1]//CONFIG['attention_heads'])\n",
    "    \n",
    "    # Global pooling to reduce sequence dimension\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.AUC()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b36696c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, fold=None):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    # Plot precision\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['precision'])\n",
    "    plt.plot(history.history['val_precision'])\n",
    "    plt.title('Model Precision')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot recall\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['recall'])\n",
    "    plt.plot(history.history['val_recall'])\n",
    "    plt.title('Model Recall')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt_path = os.path.join(CONFIG['results_dir'], 'figures', \n",
    "                           f'training_history{\"_fold\"+str(fold) if fold is not None else \"\"}.png')\n",
    "    plt.savefig(plt_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6939c5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, fold=None):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred > 0.5)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Accident'],\n",
    "                yticklabels=['Normal', 'Accident'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Save figure\n",
    "    plt_path = os.path.join(CONFIG['results_dir'], 'figures', \n",
    "                           f'confusion_matrix{\"_fold\"+str(fold) if fold is not None else \"\"}.png')\n",
    "    plt.savefig(plt_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6c21fe",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def train_with_kfold(X, y):\n",
    "    \"\"\"Train the model with k-fold cross-validation\"\"\"\n",
    "    print(f\"Starting {CONFIG['k_folds']}-fold cross-validation...\")\n",
    "    \n",
    "    # Initialize k-fold\n",
    "    kfold = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    fold_results = []\n",
    "    all_val_predictions = []\n",
    "    all_val_true = []\n",
    "    \n",
    "    # Train and evaluate for each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{CONFIG['k_folds']}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Scale features using Min-Max scaling\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_fold_reshaped = X_train_fold.reshape(-1, X_train_fold.shape[-1])\n",
    "        X_val_fold_reshaped = X_val_fold.reshape(-1, X_val_fold.shape[-1])\n",
    "        \n",
    "        X_train_fold_scaled = scaler.fit_transform(X_train_fold_reshaped)\n",
    "        X_val_fold_scaled = scaler.transform(X_val_fold_reshaped)\n",
    "        \n",
    "        # Reshape back to 3D\n",
    "        X_train_fold = X_train_fold_scaled.reshape(X_train_fold.shape)\n",
    "        X_val_fold = X_val_fold_scaled.reshape(X_val_fold.shape)\n",
    "        \n",
    "        # Build model\n",
    "        model = build_tcn_attention_model((X_train_fold.shape[1], X_train_fold.shape[2]))\n",
    "        \n",
    "        if fold == 0:\n",
    "            # Print model summary for the first fold\n",
    "            model.summary()\n",
    "            try:\n",
    "                plot_model(model, to_file=os.path.join(CONFIG['results_dir'], 'model_architecture.png'), \n",
    "                           show_shapes=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate model plot: {e}\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=CONFIG['patience'],\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                filepath=os.path.join(CONFIG['model_dir'], f'model_fold_{fold+1}.h5'),\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            epochs=CONFIG['epochs'],\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=CONFIG['class_weight']\n",
    "        )\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Plot training history\n",
    "        plot_training_history(history, fold+1)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc, val_precision, val_recall, val_auc = model.evaluate(X_val_fold, y_val_fold)\n",
    "        \n",
    "        # Get predictions\n",
    "        val_pred = model.predict(X_val_fold)\n",
    "        all_val_predictions.extend(val_pred.flatten())\n",
    "        all_val_true.extend(y_val_fold)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(y_val_fold, val_pred, fold+1)\n",
    "        \n",
    "        # Save fold results\n",
    "        fold_result = {\n",
    "            'fold': fold + 1,\n",
    "            'val_loss': float(val_loss),\n",
    "            'val_accuracy': float(val_acc),\n",
    "            'val_precision': float(val_precision),\n",
    "            'val_recall': float(val_recall),\n",
    "            'val_auc': float(val_auc),\n",
    "            'training_time': train_time,\n",
    "            'best_epoch': len(history.history['loss']) - CONFIG['patience']\n",
    "        }\n",
    "        \n",
    "        fold_results.append(fold_result)\n",
    "        print(f\"Fold {fold+1} results: {fold_result}\")\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    overall_auc = roc_auc_score(all_val_true, all_val_predictions)\n",
    "    binary_predictions = np.array(all_val_predictions) > 0.5\n",
    "    report = classification_report(all_val_true, binary_predictions, output_dict=True)\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'config': CONFIG,\n",
    "        'fold_results': fold_results,\n",
    "        'overall_auc': float(overall_auc),\n",
    "        'classification_report': report\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(CONFIG['results_dir'], 'kfold_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    # Plot overall confusion matrix\n",
    "    plot_confusion_matrix(all_val_true, all_val_predictions)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5464a1e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def train_final_model(X, y):\n",
    "    \"\"\"Train final model on all data with a proper test split\"\"\"\n",
    "    print(\"\\nTraining final model...\")\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG['test_size'], random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split train into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=CONFIG['val_size'], random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "    X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "    X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "    X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_train = X_train_scaled.reshape(X_train.shape)\n",
    "    X_val = X_val_scaled.reshape(X_val.shape)\n",
    "    X_test = X_test_scaled.reshape(X_test.shape)\n",
    "    \n",
    "    # Save scaler for future use\n",
    "    joblib.dump(scaler, os.path.join(CONFIG['model_dir'], 'scaler.pkl'))\n",
    "    \n",
    "    # Build model\n",
    "    model = build_tcn_attention_model((X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=CONFIG['patience'],\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(CONFIG['model_dir'], 'final_model.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=CONFIG['class_weight']\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_precision, test_recall, test_auc = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Get predictions\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test, test_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    binary_predictions = (test_pred > 0.5).astype(int)\n",
    "    report = classification_report(y_test, binary_predictions, output_dict=True)\n",
    "    \n",
    "    # Save test results\n",
    "    test_results = {\n",
    "        'test_loss': float(test_loss),\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_auc': float(test_auc),\n",
    "        'classification_report': report\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(CONFIG['results_dir'], 'test_results.json'), 'w') as f:\n",
    "        json.dump(test_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Test results: {test_results}\")\n",
    "    \n",
    "    # Save model in SavedModel format for deployment\n",
    "    model.save(os.path.join(CONFIG['model_dir'], 'final_model_saved'))\n",
    "    \n",
    "    print(\"Final model training complete!\")\n",
    "    \n",
    "    return model, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcd7c69",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to execute the training pipeline\"\"\"\n",
    "    # Create directories\n",
    "    create_directories()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y, feature_cols = load_and_preprocess_data()\n",
    "    \n",
    "    # Save feature columns for future reference\n",
    "    with open(os.path.join(CONFIG['model_dir'], 'feature_columns.json'), 'w') as f:\n",
    "        json.dump(feature_cols, f)\n",
    "    \n",
    "    # Train with k-fold cross-validation\n",
    "    kfold_results = train_with_kfold(X, y)\n",
    "    \n",
    "    # Train final model\n",
    "    final_model, test_results = train_final_model(X, y)\n",
    "    \n",
    "    print(\"\\nTraining pipeline complete!\")\n",
    "    print(f\"Overall AUC across folds: {kfold_results['overall_auc']:.4f}\")\n",
    "    print(f\"Final model test accuracy: {test_results['test_accuracy']:.4f}\")\n",
    "    print(f\"Final model test AUC: {test_results['test_auc']:.4f}\")\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c09d7a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Found 1211 CSV files\n",
      "Found 96 feature columns\n",
      "Created 467903 sequences\n",
      "Class distribution: Counter({0: 286124, 1: 181779})\n",
      "Accident type distribution:\n",
      "  Reactor Scram: 660\n",
      "Starting 5-fold cross-validation...\n",
      "\n",
      "Training fold 1/5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb8b26-90d9-4e6a-88d0-2d61d6d1da28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
